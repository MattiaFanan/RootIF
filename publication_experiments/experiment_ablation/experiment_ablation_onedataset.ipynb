{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOrw5Y3FtJI2"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mROexU0qa-c9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import average_precision_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import shap\n",
        "import sys\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from collections import defaultdict\n",
        "from timeit import default_timer as timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "working_dir = os.getcwd()\n",
        "import odds_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classes and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Cp5jGltPqi"
      },
      "source": [
        "## Isolation Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diffi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def local_diffi_vectorized(iforest, X_batch):\n",
        "    start_time = time.time()\n",
        "    if len(X_batch.shape) == 1:\n",
        "        X_batch = X_batch.reshape(1, -1)\n",
        "\n",
        "    n_samples, n_features = X_batch.shape\n",
        "    if n_samples == 0:\n",
        "        return np.empty((0, n_features)), 0.0\n",
        "\n",
        "    cfi_batch_total = np.zeros((n_samples, n_features), dtype=float)\n",
        "    counter_batch_total = np.zeros((n_samples, n_features), dtype=float)\n",
        "\n",
        "    if not iforest.trees:\n",
        "        return np.zeros((n_samples, n_features)), 0.0\n",
        "\n",
        "    for tree in iforest.trees:\n",
        "        if not tree.nodes: \n",
        "            continue\n",
        "        paths_matrix, path_lengths = tree.get_paths_and_path_lengths_vectorized(X_batch)\n",
        "        \n",
        "        deltas = np.zeros(n_samples, dtype=float)\n",
        "        non_zero_len_mask = path_lengths > 0\n",
        "        \n",
        "        valid_path_lengths = path_lengths[non_zero_len_mask]\n",
        "        if valid_path_lengths.size > 0:\n",
        "             deltas[non_zero_len_mask] = 1.0 / valid_path_lengths\n",
        "\n",
        "        max_depth_in_batch_for_tree = np.max(path_lengths) if path_lengths.size > 0 else 0\n",
        "        \n",
        "        for step in range(max_depth_in_batch_for_tree): \n",
        "            features_at_this_step = paths_matrix[:, step]\n",
        "            \n",
        "            valid_split_mask = features_at_this_step != -1\n",
        "            long_enough_path_mask = path_lengths > step # path_lengths is count of splits\n",
        "            \n",
        "            final_mask_for_step = valid_split_mask & long_enough_path_mask\n",
        "\n",
        "            if not np.any(final_mask_for_step):\n",
        "                continue\n",
        "\n",
        "            sample_indices_at_step = np.where(final_mask_for_step)[0]\n",
        "            actual_features_split_on = features_at_this_step[sample_indices_at_step]\n",
        "            deltas_for_these_samples = deltas[sample_indices_at_step]\n",
        "\n",
        "            np.add.at(cfi_batch_total, (sample_indices_at_step, actual_features_split_on), deltas_for_these_samples)\n",
        "            np.add.at(counter_batch_total, (sample_indices_at_step, actual_features_split_on), 1)\n",
        "            \n",
        "    fi_batch = np.zeros((n_samples, n_features), dtype=float)\n",
        "    non_zero_counter_mask = counter_batch_total > 0\n",
        "    rows, cols = np.where(non_zero_counter_mask)\n",
        "    if rows.size > 0:\n",
        "        fi_batch[rows, cols] = cfi_batch_total[rows, cols] / counter_batch_total[rows, cols]\n",
        "    \n",
        "    end_time = time.time()\n",
        "    return fi_batch, end_time - start_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ITree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qv5JiRPCbLM3"
      },
      "outputs": [],
      "source": [
        "@np.vectorize\n",
        "def c(n): \n",
        "    if n<=1: return 0\n",
        "    elif n==2: return 1 \n",
        "    else: return 2*(np.log(n-1)+np.euler_gamma-(n-1)/n)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    feature: int = None \n",
        "    threshold: float = None\n",
        "    depth: int = 0 \n",
        "\n",
        "\n",
        "class RootForcedITree: \n",
        "    def __init__(self, max_depth, new_feature=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.new_feature = new_feature\n",
        "        self.nodes = {} \n",
        "        self.used_features_in_tree = set() \n",
        "        self.features = []\n",
        "\n",
        "    def _random_split(self, X, force_f = None): \n",
        "        if force_f is None: \n",
        "            f = np.random.randint(0, X.shape[1])\n",
        "        else: \n",
        "            f = force_f\n",
        "        \n",
        "        min_val = np.min(X[:, f])\n",
        "        max_val = np.max(X[:, f])\n",
        "        if min_val == max_val: \n",
        "            threshold = min_val\n",
        "        else:\n",
        "            threshold = np.random.uniform(min_val, max_val)\n",
        "        return f, threshold\n",
        "    \n",
        "    def _build_tree(self, X, depth, node_id): \n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if X.shape[0] == 0:\n",
        "            is_X_all_same = True \n",
        "        elif X.shape[0] == 1:\n",
        "             is_X_all_same = True\n",
        "        else:\n",
        "            is_X_all_same = np.all(np.max(X, axis=0) == np.min(X, axis=0))\n",
        "\n",
        "        if n_samples <= 1 or depth >= self.max_depth or is_X_all_same: \n",
        "            leaf_depth = depth + c(n_samples)\n",
        "            self.nodes[node_id] = Node(feature=None, threshold=None, depth=leaf_depth)\n",
        "            return\n",
        "        \n",
        "        # force feature at root \n",
        "        if depth == 0 and self.new_feature is not None: \n",
        "            f, threshold = self._random_split(X, self.new_feature)\n",
        "        else:\n",
        "            f, threshold = self._random_split(X)\n",
        "        \n",
        "        self.used_features_in_tree.add(f)\n",
        "        self.nodes[node_id] = Node(f, threshold, float(depth)) \n",
        "        \n",
        "        left_mask = X[:, f] <= threshold\n",
        "        right_mask = ~left_mask \n",
        "\n",
        "        left_id, right_id = 2 * node_id + 1, 2 * node_id + 2\n",
        "        self._build_tree(X[left_mask], depth + 1, left_id)\n",
        "        self._build_tree(X[right_mask], depth + 1, right_id)\n",
        "\n",
        "    def fit(self, X): \n",
        "        self.nodes = {} \n",
        "        self.used_features_in_tree = set() \n",
        "        if X.shape[0] == 0 or X.shape[1] == 0: \n",
        "            self.nodes[0] = Node(feature=None, threshold=None, depth=c(X.shape[0]))\n",
        "            self.features = []\n",
        "            return\n",
        "\n",
        "        self._build_tree(X, 0, 0)\n",
        "        self.features = sorted(list(self.used_features_in_tree)) \n",
        "    \n",
        "    def get_depths(self, X): \n",
        "        if not self.nodes: \n",
        "            return np.zeros(X.shape[0]) + c(0) \n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        if n_samples == 0:\n",
        "            return np.array([], dtype=float)\n",
        "            \n",
        "        final_paths_depths = np.zeros(n_samples, dtype=float)\n",
        "        \n",
        "        current_node_ids = np.zeros(n_samples, dtype=int)\n",
        "        active_samples_mask = np.ones(n_samples, dtype=bool)\n",
        "        \n",
        "        for _iter_count in range(self.max_depth + 2): # Max traversal depth + safety\n",
        "            if not np.any(active_samples_mask):\n",
        "                break\n",
        "\n",
        "            active_indices = np.where(active_samples_mask)[0]\n",
        "            \n",
        "            nodes_for_active = [self.nodes[nid] for nid in current_node_ids[active_indices]]\n",
        "            \n",
        "            is_leaf_mask_local = np.array([node.feature is None for node in nodes_for_active])\n",
        "            \n",
        "            leaf_landed_global_indices = active_indices[is_leaf_mask_local]\n",
        "            if len(leaf_landed_global_indices) > 0:\n",
        "                final_paths_depths[leaf_landed_global_indices] = [nodes_for_active[i].depth for i, is_leaf in enumerate(is_leaf_mask_local) if is_leaf]\n",
        "                active_samples_mask[leaf_landed_global_indices] = False\n",
        "            \n",
        "            # Update active_indices for non-leaf samples\n",
        "            active_indices_for_split = active_indices[~is_leaf_mask_local]\n",
        "            if len(active_indices_for_split) == 0: \n",
        "                break\n",
        "\n",
        "            nodes_to_split_from = [node for i, node in enumerate(nodes_for_active) if not is_leaf_mask_local[i]]\n",
        "\n",
        "            split_features = np.array([node.feature for node in nodes_to_split_from], dtype=int)\n",
        "            split_thresholds = np.array([node.threshold for node in nodes_to_split_from])\n",
        "            \n",
        "            X_values_for_split = X[active_indices_for_split, split_features]\n",
        "            goes_left = X_values_for_split <= split_thresholds\n",
        "            \n",
        "            old_node_ids_for_split = current_node_ids[active_indices_for_split]\n",
        "            \n",
        "            left_children = 2 * old_node_ids_for_split + 1\n",
        "            right_children = 2 * old_node_ids_for_split + 2\n",
        "            \n",
        "            current_node_ids[active_indices_for_split] = np.where(goes_left, left_children, right_children)\n",
        "        return final_paths_depths\n",
        "\n",
        "    def get_paths_and_path_lengths_vectorized(self, X): # For local_diffi\n",
        "        if not self.nodes:\n",
        "             return np.full((X.shape[0], self.max_depth), -1, dtype=int), np.zeros(X.shape[0], dtype=int)\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        if n_samples == 0:\n",
        "            return np.empty((0, self.max_depth), dtype=int), np.array([], dtype=int)\n",
        "\n",
        "        paths_features_matrix = np.full((n_samples, self.max_depth), -1, dtype=int)\n",
        "        path_lengths_actual = np.zeros(n_samples, dtype=int) # Number of splits\n",
        "        \n",
        "        current_node_ids = np.zeros(n_samples, dtype=int)\n",
        "        active_samples_mask = np.ones(n_samples, dtype=bool)\n",
        "        \n",
        "        current_traversal_splits = 0 \n",
        "\n",
        "        while np.any(active_samples_mask) and current_traversal_splits < self.max_depth:\n",
        "            active_indices = np.where(active_samples_mask)[0]\n",
        "            \n",
        "            nodes_for_active = [self.nodes[nid] for nid in current_node_ids[active_indices]]\n",
        "            is_leaf_mask_local = np.array([node.feature is None for node in nodes_for_active])\n",
        "            \n",
        "            leaf_landed_global_indices = active_indices[is_leaf_mask_local]\n",
        "            if len(leaf_landed_global_indices) > 0:\n",
        "                active_samples_mask[leaf_landed_global_indices] = False\n",
        "            \n",
        "            active_indices_for_split = active_indices[~is_leaf_mask_local]\n",
        "            if len(active_indices_for_split) == 0:\n",
        "                break\n",
        "\n",
        "            nodes_to_split_from = [node for i, node in enumerate(nodes_for_active) if not is_leaf_mask_local[i]]\n",
        "\n",
        "            split_features = np.array([node.feature for node in nodes_to_split_from], dtype=int)\n",
        "            split_thresholds = np.array([node.threshold for node in nodes_to_split_from])\n",
        "            \n",
        "            paths_features_matrix[active_indices_for_split, current_traversal_splits] = split_features\n",
        "            path_lengths_actual[active_indices_for_split] += 1\n",
        "\n",
        "            X_values_for_split = X[active_indices_for_split, split_features]\n",
        "            goes_left = X_values_for_split <= split_thresholds\n",
        "            \n",
        "            old_node_ids_for_split = current_node_ids[active_indices_for_split]\n",
        "            left_children = 2 * old_node_ids_for_split + 1\n",
        "            right_children = 2 * old_node_ids_for_split + 2\n",
        "            current_node_ids[active_indices_for_split] = np.where(goes_left, left_children, right_children)\n",
        "            \n",
        "            current_traversal_splits += 1\n",
        "        \n",
        "        return paths_features_matrix, path_lengths_actual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RootForcedIForest: \n",
        "    def __init__(self, contamination=0.1, n_estimators=100, max_samples=256, max_depth=8):\n",
        "        self.contamination = contamination\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples # This is psi, the subsample size for tree building\n",
        "        self.max_depth = max_depth\n",
        "        self.threshold = None\n",
        "        self.trees = []\n",
        "    \n",
        "    def fit(self, X, new_feature=None): \n",
        "        n_total_samples = X.shape[0]\n",
        "        if n_total_samples == 0:\n",
        "            self.trees = []\n",
        "            self.threshold = 0.0 \n",
        "            return self\n",
        "\n",
        "        current_max_samples_for_subsampling = min(self.max_samples, n_total_samples)\n",
        "        if current_max_samples_for_subsampling == 0 and n_total_samples > 0:\n",
        "            current_max_samples_for_subsampling = n_total_samples\n",
        "        \n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators): \n",
        "            if current_max_samples_for_subsampling == 0 : \n",
        "                X_subsample = np.empty((0, X.shape[1]))\n",
        "            else:\n",
        "                sampled_indices = np.random.choice(n_total_samples, current_max_samples_for_subsampling, replace=True)\n",
        "                X_subsample = X[sampled_indices, :]\n",
        "            \n",
        "            tree = RootForcedITree(self.max_depth, new_feature)\n",
        "            tree.fit(X_subsample) # fit handles empty X_subsample internally\n",
        "            self.trees.append(tree)\n",
        "        \n",
        "        training_scores = self.decision_function(X) # Use all X for threshold setting\n",
        "        self._set_threshold(training_scores)\n",
        "        return self\n",
        "    \n",
        "    def _set_threshold(self, training_scores): \n",
        "        if training_scores.size == 0:\n",
        "            self.threshold = 0.0 \n",
        "        else:\n",
        "            self.threshold = np.percentile(training_scores, 100 * (1 - self.contamination))\n",
        "\n",
        "    def decision_function(self, X, diffi_weights=False):\n",
        "        n_samples_X = X.shape[0]\n",
        "        if n_samples_X == 0:\n",
        "            return np.array([])\n",
        "            \n",
        "        if not self.trees:\n",
        "             return np.ones(n_samples_X) # Max anomaly score if no trees\n",
        "\n",
        "        depths = np.zeros((n_samples_X, len(self.trees)))\n",
        "        for i, tree in enumerate(self.trees): \n",
        "            depths[:, i] = tree.get_depths(X)\n",
        "\n",
        "        if diffi_weights and X.shape[1] > 0: # diffi_weights applicable only if features exist\n",
        "            diffi_scores, _ = local_diffi_vectorized(self, X) \n",
        "        \n",
        "            sum_diffi_per_sample = np.sum(diffi_scores, axis=1, keepdims=True)\n",
        "            normalized_diffi_scores = np.zeros_like(diffi_scores)\n",
        "            valid_sum_mask = (sum_diffi_per_sample > 1e-9)[:,0] # Avoid near-zero sums\n",
        "            if np.any(valid_sum_mask):\n",
        "                normalized_diffi_scores[valid_sum_mask] = diffi_scores[valid_sum_mask] / sum_diffi_per_sample[valid_sum_mask]\n",
        "\n",
        "            n_total_features = X.shape[1]\n",
        "            features_per_tree_mask = np.zeros((len(self.trees), n_total_features), dtype=bool)\n",
        "            for tree_idx, tree in enumerate(self.trees):\n",
        "                if tree.features: \n",
        "                    features_per_tree_mask[tree_idx, tree.features] = True \n",
        "            \n",
        "            trees_per_feature = np.sum(features_per_tree_mask, axis=0, dtype=float)\n",
        "            safe_trees_per_feature = np.where(trees_per_feature == 0, 1.0, trees_per_feature)\n",
        "\n",
        "            partial_diffi_score_per_feature = normalized_diffi_scores / safe_trees_per_feature[np.newaxis, :]\n",
        "            # Zero out scores for features not used in any tree (where safe_trees_per_feature was 1, original 0)\n",
        "            partial_diffi_score_per_feature[:, trees_per_feature == 0] = 0.0\n",
        "            \n",
        "            weight_per_tree = np.dot(partial_diffi_score_per_feature, features_per_tree_mask.astype(float).T)\n",
        "            \n",
        "            sum_weights_per_sample = np.sum(weight_per_tree, axis=1, keepdims=True)\n",
        "            # Normalize weights to sum to 1 for each sample\n",
        "            normalized_weight_per_tree = np.zeros_like(weight_per_tree)\n",
        "            valid_weight_sum_mask = (sum_weights_per_sample > 1e-9)[:,0]\n",
        "            if np.any(valid_weight_sum_mask):\n",
        "                 normalized_weight_per_tree[valid_weight_sum_mask] = weight_per_tree[valid_weight_sum_mask] / sum_weights_per_sample[valid_weight_sum_mask]\n",
        "            else: # If all weights are zero for a sample, use equal weighting for trees for that sample\n",
        "                 normalized_weight_per_tree = np.full_like(weight_per_tree, 1.0 / len(self.trees) if len(self.trees) > 0 else 0.0)\n",
        "\n",
        "\n",
        "            avg_depths = np.sum(depths * normalized_weight_per_tree, axis=1)\n",
        "    \n",
        "        else: \n",
        "            avg_depths = np.mean(depths, axis=1)\n",
        "        \n",
        "        if self.max_samples <= 1: # psi <= 1\n",
        "            c_psi = 0 \n",
        "        else:\n",
        "            c_psi = c(self.max_samples)\n",
        "\n",
        "        if c_psi == 0: \n",
        "            # When c_psi is 0 (e.g. self.max_samples=1), scores are 1 if avg_depth is 0, otherwise ambiguous.\n",
        "            # Typically, path length for a single sample leaf is 0, leading to score 1.\n",
        "            scores = np.ones_like(avg_depths)\n",
        "            # If avg_depth > 0 and c_psi = 0, it's an edge case.\n",
        "            # Score might be 0 (least anomalous) or handled differently.\n",
        "            # For safety, if depth > 0 it means not immediately isolated.\n",
        "            scores[avg_depths > 0] = 0.0 # less anomalous if it took some path\n",
        "            return scores\n",
        "        else:\n",
        "            return 2 ** (-avg_depths / c_psi)\n",
        "    \n",
        "    def predict(self, X, diffi_weights=False): \n",
        "        if self.threshold is None: \n",
        "            raise ValueError(\"Model has not been fitted yet or threshold not set.\")\n",
        "        scores = self.decision_function(X, diffi_weights)\n",
        "        return (scores >= self.threshold).astype(int) # Corrected: >= threshold for anomaly (consistent with percentile)\n",
        "\n",
        "    def update(self, teacher, keep_size=False):\n",
        "        # Make copies to avoid modifying original teacher's trees\n",
        "        # and to have a distinct list of current trees before modification\n",
        "        current_own_trees = list(self.trees) # Shallow copy of the list of trees\n",
        "        new_teacher_trees = copy.deepcopy(teacher.trees)\n",
        "\n",
        "        if not keep_size:\n",
        "            # If not keeping size, simply extend with all new trees\n",
        "            self.trees.extend(new_teacher_trees)\n",
        "        else:\n",
        "            # If keeping size, the target is self.n_estimators (original capacity)\n",
        "            target_n_estimators = self.n_estimators\n",
        "\n",
        "            # Combine all potential candidate trees (old and new)\n",
        "            all_candidate_trees = current_own_trees + new_teacher_trees\n",
        "            num_candidates = len(all_candidate_trees)\n",
        "\n",
        "            if num_candidates <= target_n_estimators:\n",
        "                # If the combined pool is within or equal to the target capacity, keep all\n",
        "                self.trees = all_candidate_trees\n",
        "            else:\n",
        "                num_own_trees = len(current_own_trees)\n",
        "                num_new_trees = len(new_teacher_trees)\n",
        "                p_own = np.ones(num_own_trees)*num_new_trees\n",
        "                p_new = np.ones(num_new_trees)*num_own_trees\n",
        "                p = np.concatenate((p_own, p_new))\n",
        "                p = p / np.sum(p)\n",
        "                # If the combined pool exceeds target capacity,\n",
        "                # uniformly sample target_n_estimators from the combined pool.\n",
        "                # Each tree (old or new) has an equal chance of being selected.\n",
        "                chosen_indices = np.random.choice(\n",
        "                    num_candidates,\n",
        "                    target_n_estimators,\n",
        "                    p=p,\n",
        "                    replace=False  # We want unique trees\n",
        "                )\n",
        "                self.trees = [all_candidate_trees[i] for i in chosen_indices]\n",
        "        \n",
        "        # Update the actual number of estimators currently in the model\n",
        "        self.n_estimators = len(self.trees)\n",
        "\n",
        "        # Update other parameters from teacher\n",
        "        self.max_samples = max(teacher.max_samples, self.max_samples)\n",
        "        self.threshold = copy.copy(teacher.threshold)\n",
        "        self.max_depth = max(teacher.max_depth, self.max_depth)\n",
        "        self.contamination = copy.copy(teacher.contamination)\n",
        "        \n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMR88ch7nvrC"
      },
      "source": [
        "## Dataset slicing sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def incremental_dataset_sizes(n_total, x_min, k):\n",
        "    \"\"\"\n",
        "    Compute dataset increments s_i such that log(cumulative size) grows linearly.\n",
        "\n",
        "    Args:\n",
        "        T (int): Number of steps.\n",
        "        K_final (float): Final total dataset size after T steps.\n",
        "\n",
        "    Returns:\n",
        "        list of int: Dataset sizes to add at each step.\n",
        "    \"\"\"\n",
        "    alpha = np.log(n_total) / (k-1)\n",
        "    i_vals = np.arange(k)\n",
        "    K_vals = np.exp(alpha * i_vals)\n",
        "    s_vals = np.diff(K_vals, prepend=0)\n",
        "    s_vals = np.round(s_vals).astype(int).tolist()\n",
        "    s_vals[0] = max(x_min, s_vals[0])  # Ensure first dataset has enough points\n",
        "    s_vals = [max(1, s) for s in s_vals]  # Ensure all increments are at least 1\n",
        "    return s_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1466\n",
            "[2, 1, 3, 6, 14, 32, 72, 161, 362, 813]\n"
          ]
        }
      ],
      "source": [
        "print(sum(incremental_dataset_sizes(1464, 2, 10)))\n",
        "print(incremental_dataset_sizes(1464, 2, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        try:\n",
        "            import os\n",
        "            os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            import random\n",
        "            random.seed(seed)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            np.random.seed(seed)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to avoid keyerror in dict[k1][k2]=v if k1 not in dict\n",
        "def nested_defaultdict():\n",
        "    return defaultdict(nested_defaultdict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IPvOYMMOsRKW"
      },
      "outputs": [],
      "source": [
        "def run_isolation_forest_experiment(\n",
        "    X, y,\n",
        "    num_experiences,\n",
        "    num_blocks_per_experience,\n",
        "    num_test_points,\n",
        "    n_new_trees,\n",
        "    n_start_trees=100,\n",
        "    contamination=0.1,\n",
        "    random_state=None,\n",
        "    feature_permutation=None,\n",
        "    model_class=None\n",
        "):\n",
        "    if model_class is None:\n",
        "        raise ValueError(\"You must provide a model class, e.g., RootForcedIsolationForest.\")\n",
        "    \n",
        "    num_features = X.shape[1]\n",
        "    \n",
        "    if num_experiences >= num_features:\n",
        "        raise ValueError(\"The number of experiences must be less than the number of features.\")\n",
        "\n",
        "    y_mapped = np.array(y)\n",
        "\n",
        "    # baseline 1\n",
        "    fixed_ap = nested_defaultdict()\n",
        "    fixed_update_time = nested_defaultdict()\n",
        "    fixed_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    fixed_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # baseline 2\n",
        "    from_scratch_ap = nested_defaultdict()\n",
        "    from_scratch_update_time = nested_defaultdict()\n",
        "    from_scratch_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    from_scratch_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # just to check the seeds setting\n",
        "    from_scratch_2_ap = nested_defaultdict()\n",
        "    from_scratch_2_update_time = nested_defaultdict()\n",
        "    from_scratch_2_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    from_scratch_2_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # just to check a normal isolation forest\n",
        "    from_scratch_iso_ap = nested_defaultdict()\n",
        "    from_scratch_iso_update_time = nested_defaultdict()\n",
        "    from_scratch_iso_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    from_scratch_iso_model = IsolationForest(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # cartesian product of:\n",
        "    # diffi [True, False]\n",
        "    # keep_size [True, False]\n",
        "    # force_feature [True, False]\n",
        "\n",
        "    # base\n",
        "    ours_none_ap = nested_defaultdict()\n",
        "    ours_none_update_time = nested_defaultdict()\n",
        "    ours_none_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_none_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # base + one new thing\n",
        "    ours_force_ap = nested_defaultdict()\n",
        "    ours_force_update_time = nested_defaultdict()\n",
        "    ours_force_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_force_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    ours_diffi_ap = nested_defaultdict()\n",
        "    ours_diffi_update_time = nested_defaultdict()\n",
        "    ours_diffi_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_diffi_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    ours_keepsize_ap = nested_defaultdict()\n",
        "    ours_keepsize_update_time = nested_defaultdict()\n",
        "    ours_keepsize_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_keepsize_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # base + two new things\n",
        "\n",
        "    ours_force_diffi_ap = nested_defaultdict()\n",
        "    ours_force_diffi_update_time = nested_defaultdict()\n",
        "    ours_force_diffi_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_force_diffi_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    ours_keepsize_diffi_ap = nested_defaultdict()\n",
        "    ours_keepsize_diffi_update_time = nested_defaultdict()\n",
        "    ours_keepsize_diffi_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_keepsize_diffi_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    ours_force_keepsize_ap = nested_defaultdict()\n",
        "    ours_force_keepsize_update_time = nested_defaultdict()\n",
        "    ours_force_keepsize_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_force_keepsize_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    # all together\n",
        "    ours_all_ap = nested_defaultdict()\n",
        "    ours_all_update_time = nested_defaultdict()\n",
        "    ours_all_inference_time = nested_defaultdict()\n",
        "    set_seeds(random_state)\n",
        "    ours_all_model = model_class(contamination=contamination, n_estimators=n_start_trees)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_mapped,\n",
        "            test_size=num_test_points,\n",
        "            stratify=y_mapped,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "    # permutation of the features\n",
        "    if feature_permutation is not None:\n",
        "        features_exp = feature_permutation\n",
        "    else:\n",
        "        features_exp = np.arange(num_features)\n",
        "    features_exp = [features_exp[:len(features_exp)-i] for i in reversed(list(range(min(num_features, num_experiences))))]\n",
        "    \n",
        "    train_datasets = []\n",
        "    x_tail = X_train\n",
        "    y_tail = y_train\n",
        "    block_samples = incremental_dataset_sizes(x_tail.shape[0], x_min=2, k=num_blocks_per_experience) \n",
        "    for i, n_samples in enumerate(block_samples[:-1]): # remove last since it is the last y_tail\n",
        "        x_tail, x_head, y_tail, y_head = train_test_split(\n",
        "            x_tail,\n",
        "            y_tail,\n",
        "            test_size=n_samples,\n",
        "            #stratify=y_tail,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        train_datasets.append(x_head)\n",
        "        if x_tail.shape[0] < block_samples[i+1]:\n",
        "            # too few samples in the dataset, just cut the blocks_per_experience\n",
        "            break\n",
        "    train_datasets.append(x_tail)\n",
        "\n",
        "\n",
        "    for experience, selected_features in enumerate(features_exp):\n",
        "        \n",
        "        accumulated_train_dataset = np.empty((0, X_train.shape[1]))\n",
        "        x_test_exp = X_test[..., selected_features]\n",
        "\n",
        "        for block in tqdm(range(len(train_datasets)), total=len(train_datasets)):\n",
        "            accumulated_train_dataset = np.concatenate((accumulated_train_dataset, train_datasets[block]), axis=0)\n",
        "            block_train_dataset = accumulated_train_dataset[..., selected_features]\n",
        "            atd_size = accumulated_train_dataset.shape[0]\n",
        "\n",
        "            # from scratch model is refitted every time in any case\n",
        "            set_seeds(random_state)\n",
        "            t = timer()\n",
        "            from_scratch_model.fit(block_train_dataset)\n",
        "            from_scratch_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            set_seeds(random_state)\n",
        "            t = timer()\n",
        "            from_scratch_2_model.fit(block_train_dataset)\n",
        "            from_scratch_2_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            set_seeds(random_state)\n",
        "            t = timer()\n",
        "            from_scratch_iso_model.fit(block_train_dataset)\n",
        "            from_scratch_iso_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "            # make a copy of the model\n",
        "            # \"for block\" is used to assert the influence of the training size, so the model is updated only after the last block\"\n",
        "            exp_ours_none_model = deepcopy(ours_none_model)\n",
        "            exp_ours_force_model = deepcopy(ours_force_model)\n",
        "            exp_ours_diffi_model = deepcopy(ours_diffi_model)\n",
        "            exp_ours_keepsize_model = deepcopy(ours_keepsize_model)\n",
        "            exp_ours_force_diffi_model = deepcopy(ours_force_diffi_model)\n",
        "            exp_ours_keepsize_diffi_model = deepcopy(ours_keepsize_diffi_model)\n",
        "            exp_ours_force_keepsize_model = deepcopy(ours_force_keepsize_model)\n",
        "            exp_ours_all_model = deepcopy(ours_all_model)\n",
        "\n",
        "            if experience == 0:\n",
        "                # the first experience is the same for all models, as there is no previous info and no forced feature\n",
        "                # and inside of each experience only the training dataset size influence is asserted\n",
        "\n",
        "                # 8 + fixed \n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                fixed_model.fit(block_train_dataset)\n",
        "                fixed_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_none_model.fit(block_train_dataset)\n",
        "                ours_none_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_model.fit(block_train_dataset)\n",
        "                ours_force_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_diffi_model.fit(block_train_dataset)\n",
        "                ours_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_keepsize_model.fit(block_train_dataset)\n",
        "                ours_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_diffi_model.fit(block_train_dataset)\n",
        "                ours_force_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_keepsize_diffi_model.fit(block_train_dataset)\n",
        "                ours_keepsize_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_keepsize_model.fit(block_train_dataset)\n",
        "                ours_force_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_all_model.fit(block_train_dataset)\n",
        "                ours_all_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                \n",
        "            else:\n",
        "                # fixed model not updated \n",
        "                fixed_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = 0\n",
        "                # 8 ours_models\n",
        "                # build with n_estimators=n_new_trees, since tmp models are used only to add new trees\n",
        "                pos_new_feature = block_train_dataset.shape[1]-1\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_none_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_none_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_none_model.fit(block_train_dataset)\n",
        "                ours_none_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_none_model.update(tmp_ours_none_model)\n",
        "                ours_none_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                \n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_force_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_model.fit(block_train_dataset, new_feature=pos_new_feature)\n",
        "                ours_force_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_model.update(tmp_ours_force_model)\n",
        "                ours_force_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_diffi_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_diffi_model.fit(block_train_dataset)\n",
        "                ours_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_diffi_model.update(tmp_ours_diffi_model)\n",
        "                ours_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_keepsize_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"]= timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_keepsize_model.fit(block_train_dataset)\n",
        "                ours_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_keepsize_model.update(tmp_ours_keepsize_model, keep_size=True)\n",
        "                ours_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_diffi_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_force_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_diffi_model.fit(block_train_dataset, new_feature=pos_new_feature)\n",
        "                ours_force_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_diffi_model.update(tmp_ours_force_diffi_model)\n",
        "                ours_force_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_keepsize_diffi_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_keepsize_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_keepsize_diffi_model.fit(block_train_dataset)\n",
        "                ours_keepsize_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_keepsize_diffi_model.update(tmp_ours_keepsize_diffi_model, keep_size=True)\n",
        "                ours_keepsize_diffi_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_keepsize_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_force_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_force_keepsize_model.fit(block_train_dataset, new_feature=pos_new_feature)\n",
        "                ours_force_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_force_keepsize_model.update(tmp_ours_force_keepsize_model, keep_size=True)\n",
        "                ours_force_keepsize_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_all_model = model_class(contamination=contamination, n_estimators=n_new_trees)\n",
        "                ours_all_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                tmp_ours_all_model.fit(block_train_dataset, new_feature=pos_new_feature)\n",
        "                ours_all_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "                set_seeds(random_state)\n",
        "                t = timer()\n",
        "                exp_ours_all_model.update(tmp_ours_all_model, keep_size=True)\n",
        "                ours_all_update_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] += timer() - t\n",
        "\n",
        "            if block == len(train_datasets) - 1:\n",
        "                # update the safe copy of the models\n",
        "                # 8 ours_models\n",
        "                ours_none_model = copy.deepcopy(exp_ours_none_model)\n",
        "                ours_force_model = copy.deepcopy(exp_ours_force_model)\n",
        "                ours_diffi_model = copy.deepcopy(exp_ours_diffi_model)\n",
        "                ours_keepsize_model = copy.deepcopy(exp_ours_keepsize_model)\n",
        "                ours_force_diffi_model = copy.deepcopy(exp_ours_force_diffi_model)\n",
        "                ours_keepsize_diffi_model = copy.deepcopy(exp_ours_keepsize_diffi_model)\n",
        "                ours_force_keepsize_model = copy.deepcopy(exp_ours_force_keepsize_model)\n",
        "                ours_all_model = copy.deepcopy(exp_ours_all_model)\n",
        "            \n",
        "            #### TEST\n",
        "            # 3 from scratch models + fixed + 8 ours_models = 12 models\n",
        "            # there is no randomness in the test, so we can avoid setting seeds here\n",
        "            t = timer()\n",
        "            from_scratch_scores = from_scratch_model.decision_function(x_test_exp)\n",
        "            from_scratch_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            from_scratch_model_2_scores = from_scratch_2_model.decision_function(x_test_exp)\n",
        "            from_scratch_2_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            iso_scores = -from_scratch_iso_model.decision_function(x_test_exp)\n",
        "            from_scratch_iso_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            \n",
        "            t = timer()\n",
        "            fixed_scores = fixed_model.decision_function(x_test_exp)\n",
        "            fixed_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "            t = timer()\n",
        "            ours_none_model_scores = exp_ours_none_model.decision_function(x_test_exp)\n",
        "            ours_none_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_force_model_scores = exp_ours_force_model.decision_function(x_test_exp)\n",
        "            ours_force_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_diffi_model_scores = exp_ours_diffi_model.decision_function(x_test_exp, diffi_weights=True)\n",
        "            ours_diffi_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_keepsize_model_scores = exp_ours_keepsize_model.decision_function(x_test_exp)\n",
        "            ours_keepsize_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_force_diffi_model_scores = exp_ours_force_diffi_model.decision_function(x_test_exp, diffi_weights=True)\n",
        "            ours_force_diffi_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_keepsize_diffi_model_scores = exp_ours_keepsize_diffi_model.decision_function(x_test_exp, diffi_weights=True)\n",
        "            ours_keepsize_diffi_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_force_keepsize_model_scores = exp_ours_force_keepsize_model.decision_function(x_test_exp)\n",
        "            ours_force_keepsize_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "            t = timer()\n",
        "            ours_all_model_scores = exp_ours_all_model.decision_function(x_test_exp, diffi_weights=True)\n",
        "            ours_all_inference_time[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = timer() - t\n",
        "\n",
        "            # Metrics\n",
        "            from_scratch_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, from_scratch_scores)\n",
        "            from_scratch_2_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, from_scratch_model_2_scores)\n",
        "            from_scratch_iso_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, iso_scores)\n",
        "            fixed_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"]= average_precision_score(y_test, fixed_scores)\n",
        "\n",
        "            ours_none_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_none_model_scores)\n",
        "            ours_force_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_force_model_scores)\n",
        "            ours_diffi_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_diffi_model_scores)\n",
        "            ours_keepsize_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_keepsize_model_scores)\n",
        "            ours_force_diffi_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_force_diffi_model_scores)\n",
        "            ours_keepsize_diffi_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_keepsize_diffi_model_scores)\n",
        "            ours_force_keepsize_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_force_keepsize_model_scores)\n",
        "            ours_all_ap[f\"n_features:{len(selected_features)}\"][f\"train_set_size:{atd_size}\"] = average_precision_score(y_test, ours_all_model_scores)\n",
        "\n",
        "\n",
        "    main_output = {\n",
        "        \"fixed\": {\n",
        "            \"average_precision\": fixed_ap,\n",
        "            \"update_time\": fixed_update_time,\n",
        "            \"inference_time\": fixed_inference_time\n",
        "        },\n",
        "        \"from_scratch\": {\n",
        "            \"average_precision\": from_scratch_ap,\n",
        "            \"update_time\": from_scratch_update_time,\n",
        "            \"inference_time\": from_scratch_inference_time\n",
        "        },\n",
        "        \"ours_none\": {\n",
        "            \"average_precision\": ours_none_ap,\n",
        "            \"update_time\": ours_none_update_time,\n",
        "            \"inference_time\": ours_none_inference_time\n",
        "        },\n",
        "        \"ours_force\": {\n",
        "            \"average_precision\": ours_force_ap,\n",
        "            \"update_time\": ours_force_update_time,\n",
        "            \"inference_time\": ours_force_inference_time\n",
        "        },\n",
        "        \"ours_diffi\": {\n",
        "            \"average_precision\": ours_diffi_ap,\n",
        "            \"update_time\": ours_diffi_update_time,\n",
        "            \"inference_time\": ours_diffi_inference_time\n",
        "        },\n",
        "        \"ours_keepsize\": {\n",
        "            \"average_precision\": ours_keepsize_ap,\n",
        "            \"update_time\": ours_keepsize_update_time,\n",
        "            \"inference_time\": ours_keepsize_inference_time\n",
        "        },\n",
        "        \"ours_force_diffi\": {\n",
        "            \"average_precision\": ours_force_diffi_ap,\n",
        "            \"update_time\": ours_force_diffi_update_time,\n",
        "            \"inference_time\": ours_force_diffi_inference_time\n",
        "        },\n",
        "        \"ours_keepsize_diffi\": {\n",
        "            \"average_precision\": ours_keepsize_diffi_ap,\n",
        "            \"update_time\": ours_keepsize_diffi_update_time,\n",
        "            \"inference_time\": ours_keepsize_diffi_inference_time\n",
        "        },\n",
        "        \"ours_force_keepsize\": {\n",
        "            \"average_precision\": ours_force_keepsize_ap,\n",
        "            \"update_time\": ours_force_keepsize_update_time,\n",
        "            \"inference_time\": ours_force_keepsize_inference_time\n",
        "        },\n",
        "        \"ours_all\": {\n",
        "            \"average_precision\": ours_all_ap,\n",
        "            \"update_time\": ours_all_update_time,\n",
        "            \"inference_time\": ours_all_inference_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "    secondary_output = {\n",
        "        \"from_scratch\": deepcopy(main_output[\"from_scratch\"]),\n",
        "        \"from_scratch_2\": {\n",
        "            \"average_precision\": from_scratch_2_ap,\n",
        "            \"update_time\": from_scratch_2_update_time,\n",
        "            \"inference_time\": from_scratch_2_inference_time\n",
        "        },\n",
        "        \"from_scratch_iso\": {\n",
        "            \"average_precision\": from_scratch_iso_ap,\n",
        "            \"update_time\": from_scratch_iso_update_time,\n",
        "            \"inference_time\": from_scratch_iso_inference_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return main_output, secondary_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1iNC1bL7t6z"
      },
      "source": [
        "## Function to make plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kwk8Z1p66gRf"
      },
      "outputs": [],
      "source": [
        "def plot_experiment_results(results, save_dir, dataset_name, model_colors=None, model_lines=None):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=False)  # Create directory if it doesn't exist\n",
        "\n",
        "    # new_trees -> seeds -> model -> metric -> n_features -> block \n",
        "\n",
        "    new_trees_keys = list(results.keys())\n",
        "    seed_keys = list(results[new_trees_keys[0]].keys())\n",
        "    models_keys = list(results[new_trees_keys[0]][seed_keys[0]].keys())\n",
        "    if model_colors is None:\n",
        "        model_colors = {model: plt.cm.tab10(i) for i, model in enumerate(models_keys)}\n",
        "    if model_lines is None:\n",
        "        model_lines = {model: '-' for model in models_keys}\n",
        "    metrics_keys = list(results[new_trees_keys[0]][seed_keys[0]][models_keys[0]].keys())\n",
        "\n",
        "    # each new_trees a plot\n",
        "    for new_trees_k in new_trees_keys:\n",
        "        n_tree_results = results[new_trees_k]\n",
        "        # each metric a plot\n",
        "        for metric in metrics_keys:\n",
        "            fig_mean, ax_mean = plt.subplots(figsize=(12, 6))\n",
        "            fig_err, ax_err = plt.subplots(figsize=(12, 6))\n",
        "            fig_std, ax_std = plt.subplots(figsize=(12, 6))\n",
        "            # each model a track [concatenate blocks in exp]\n",
        "            # average all seeds\n",
        "            n_new_trees = int(new_trees_k.split(':')[1])  # Extract the number of new trees from the key\n",
        "            title_prefix = f\"{dataset_name} - {metric} - {n_new_trees} new trees\"\n",
        "            title_prefix = title_prefix.replace('_', ' ').title()\n",
        "            x_ticks_labels_exp = None\n",
        "            x_ticks_labels_blocks = None\n",
        "            min_n_features = None\n",
        "            for model in models_keys:\n",
        "                x = []\n",
        "                y = []\n",
        "                for seed in seed_keys:\n",
        "                    model_results = n_tree_results[seed][model][metric]\n",
        "                    # Concatenate all blocks for this model and seed\n",
        "                    x_exp = []\n",
        "                    y_exp = []\n",
        "                    if min_n_features is None:\n",
        "                        min_n_features = min([int(k.split(':')[1]) for k in model_results.keys()])\n",
        "                    for exp in model_results:\n",
        "                        block_lengths = list(model_results[exp].keys())\n",
        "                        x_exp.extend(sorted([int(block.split(':')[1]) for block in block_lengths]))\n",
        "                        y_exp.extend([model_results[exp][block] for block in block_lengths])\n",
        "                    x.append(x_exp)\n",
        "                    y.append(y_exp)\n",
        "\n",
        "                y = np.array(y)\n",
        "                if \"time\" in metric:\n",
        "                    y = y * 1000  # Convert to milliseconds if it's a time metric\n",
        "                y_mean = np.mean(y, axis=0)\n",
        "                # standaerd error for graph, use when the important value is the mean\n",
        "                # use std if the important part is where the values are\n",
        "                y_std = np.std(y, axis=0, ddof=1)\n",
        "                y_ste = y_std / np.sqrt(y.shape[0])\n",
        "                x = x[0]\n",
        "\n",
        "                if x_ticks_labels_blocks is None:\n",
        "                    x_ticks_labels_blocks = [str(v) for v in x]\n",
        "                if x_ticks_labels_exp is None:\n",
        "                    x_ticks_labels_exp = []\n",
        "                    i=min_n_features\n",
        "                    for val in x:\n",
        "                        if val == x[0]:\n",
        "                            x_ticks_labels_exp.append(str(i))\n",
        "                            i += 1\n",
        "                        else:\n",
        "                            x_ticks_labels_exp.append(\"\")\n",
        "                \n",
        "                ax_mean.plot(y_mean, label=model, color=model_colors[model], linestyle=model_lines[model])\n",
        "                ax_err.plot(y_mean, label=model, color=model_colors[model], linestyle=model_lines[model])\n",
        "                ax_err.fill_between(\n",
        "                    np.arange(len(y_mean)),\n",
        "                    y_mean - y_ste,\n",
        "                    y_mean + y_ste,\n",
        "                    alpha=0.1,  # transparency for the halo\n",
        "                    color=model_colors[model]\n",
        "                )\n",
        "                ax_std.plot(y_mean, label=model, color=model_colors[model], linestyle=model_lines[model])\n",
        "                ax_std.fill_between(\n",
        "                    np.arange(len(y_mean)),\n",
        "                    y_mean - y_std,\n",
        "                    y_mean + y_std,\n",
        "                    alpha=0.1,  # transparency for the halo\n",
        "                    color=model_colors[model]\n",
        "                )\n",
        "\n",
        "            ax_mean.set_xlabel(\"Training Set Length\")\n",
        "            ax_mean.set_xticks(np.arange(len(y_mean)))\n",
        "            ax_mean.set_xlim((0, len(y_mean) - 1))\n",
        "            ax_mean.set_xticklabels(x_ticks_labels_blocks, rotation=45)\n",
        "            ax_mean.set_ylabel(metric.replace('_', ' ').title() + (\" [ms]\" if \"time\" in metric else \" [a.u.]\"))\n",
        "            ax_mean.legend(loc='upper left', bbox_to_anchor=(0, 1))\n",
        "            ax_mean.set_title(title_prefix)\n",
        "            ax_mean.grid(True, linewidth=0.1)\n",
        "            # second x axis\n",
        "            ax_mean_twiny = ax_mean.twiny()     \n",
        "            ax_mean_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            # allign with the other axis\n",
        "            ax_mean_twiny.set_xlim(ax_mean.get_xlim())\n",
        "            ax_mean_twiny.set_xticks(ax_mean.get_xticks())\n",
        "            ax_mean_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            ax_mean_twiny.tick_params(axis='x', bottom=False, top=True)\n",
        "            ticks = ax_mean_twiny.xaxis.get_major_ticks()\n",
        "            for i,tick in enumerate(ticks):\n",
        "                if x_ticks_labels_exp[i] == \"\":\n",
        "                    tick.tick2line.set_visible(False)\n",
        "                    tick.label2.set_visible(False)\n",
        "            ax_mean_twiny.set_xlabel(\"Number of Features\")\n",
        "            fig_mean.savefig(\n",
        "                os.path.join(save_dir, f\"{title_prefix}.pdf\"), \n",
        "                bbox_inches='tight',\n",
        "                format='pdf'\n",
        "            )\n",
        "\n",
        "            ax_err.set_xlabel(\"Training Set Length\")\n",
        "            ax_err.set_xticks(np.arange(len(y_mean)))\n",
        "            ax_err.set_xlim((0, len(y_mean) - 1))\n",
        "            ax_err.set_xticklabels(x_ticks_labels_blocks, rotation=45)\n",
        "            ax_err.set_ylabel(metric.replace('_', ' ').title() + (\" [ms]\" if \"time\" in metric else \" [a.u.]\"))\n",
        "            ax_err.legend(loc='upper left', bbox_to_anchor=(0, 1))\n",
        "            ax_err.set_title(title_prefix)\n",
        "            ax_err.grid(True, linewidth=0.1)\n",
        "            # second x axis\n",
        "            ax_err_twiny = ax_err.twiny()\n",
        "            ax_err_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            # allign with the other axis\n",
        "            ax_err_twiny.set_xlim(ax_err.get_xlim())\n",
        "            ax_err_twiny.set_xticks(ax_err.get_xticks())\n",
        "            ax_err_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            ax_err_twiny.tick_params(axis='x', bottom=False, top=True)\n",
        "            ticks = ax_err_twiny.xaxis.get_major_ticks()\n",
        "            for i, tick in enumerate(ticks):\n",
        "                if x_ticks_labels_exp[i] == \"\":\n",
        "                    tick.tick2line.set_visible(False)\n",
        "                    tick.label2.set_visible(False)\n",
        "            ax_err_twiny.set_xlabel(\"Number of Features\")\n",
        "            fig_err.savefig(\n",
        "                os.path.join(save_dir, f\"{title_prefix}_err.pdf\"),\n",
        "                bbox_inches='tight',\n",
        "                format='pdf'\n",
        "            )\n",
        "\n",
        "            ax_std.set_xlabel(\"Training Set Length\")\n",
        "            ax_std.set_xticks(np.arange(len(y_mean)))\n",
        "            ax_std.set_xlim((0, len(y_mean) - 1))\n",
        "            ax_std.set_xticklabels(x_ticks_labels_blocks, rotation=45)\n",
        "            ax_std.set_ylabel(metric.replace('_', ' ').title() + (\" [ms]\" if \"time\" in metric else \" [a.u.]\"))\n",
        "            ax_std.legend(loc='upper left', bbox_to_anchor=(0, 1))\n",
        "            ax_std.set_title(title_prefix)\n",
        "            ax_std.grid(True, linewidth=0.1)\n",
        "            # second x axis\n",
        "            ax_std_twiny = ax_std.twiny()\n",
        "            ax_std_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            # allign with the other axis\n",
        "            ax_std_twiny.set_xlim(ax_std.get_xlim())\n",
        "            ax_std_twiny.set_xticks(ax_std.get_xticks())\n",
        "            ax_std_twiny.set_xticklabels(x_ticks_labels_exp)\n",
        "            ax_std_twiny.tick_params(axis='x', bottom=False, top=True)\n",
        "            ticks = ax_std_twiny.xaxis.get_major_ticks()\n",
        "            for i, tick in enumerate(ticks):\n",
        "                if x_ticks_labels_exp[i] == \"\":\n",
        "                    tick.tick2line.set_visible(False)\n",
        "                    tick.label2.set_visible(False)\n",
        "            ax_std_twiny.set_xlabel(\"Number of Features\")\n",
        "            fig_std.savefig(\n",
        "                os.path.join(save_dir, f\"{title_prefix}_std.pdf\"),\n",
        "                bbox_inches='tight',\n",
        "                format='pdf'\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To check if an Isolation forest would perform well enough in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def isolation_forest_oneshot_AP_check(X,y, test_ratio):\n",
        "    iforest = IsolationForest(n_estimators=100)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_ratio,\n",
        "        stratify=y\n",
        "    )\n",
        "    iforest.fit(X_train)\n",
        "    scores = - iforest.score_samples(X_test)\n",
        "    ap = average_precision_score(y_test, scores)\n",
        "    return ap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shap importance rankings\n",
        "\n",
        "We need to add meaningfull features, it make non sense for the company to purchase/install/integrate a useless sensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shaptree_feature_ranking(X, y, random_states, test_ratio=0.2):\n",
        "    n_samples, n_features = X.shape\n",
        "    feature_importance = np.zeros(n_features)\n",
        "    for seed in tqdm(random_states, total=len(random_states)):\n",
        "        iforest = IsolationForest(n_estimators=100, random_state=seed)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,  \n",
        "            test_size=int(n_samples * test_ratio),\n",
        "            stratify=y,  \n",
        "            random_state=seed\n",
        "        )\n",
        "        # select only anomalies to compute importance\n",
        "        only_anomalies_test_index = np.where(y_test == 1)[0]\n",
        "        X_test = X_test[only_anomalies_test_index, ...]\n",
        "\n",
        "        iforest.fit(X_train)\n",
        "        explainer = shap.TreeExplainer(iforest)\n",
        "        shap_values = explainer.shap_values(X_test)\n",
        "        feature_importance += np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "    feature_importance /= len(random_states)\n",
        "    ranking = pd.Series(feature_importance, index=np.arange(n_features)).sort_values(ascending=False)\n",
        "    return ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General check for rankings\n",
        "\n",
        "If there are useless features, i.e. the bottom ranking features have very low imporance, then if removing the top-k features should make the performance drop compared to removing the bottom-k ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_remove_best_worst_ranking(X, y, ranking, test_ratio, random_states, remove_k, logger):\n",
        "    n_samples, n_features = X.shape\n",
        "    if n_features != len(ranking):\n",
        "        raise ValueError(\"The number of features in X does not match the length of the ranking.\")\n",
        "    if remove_k > n_features // 2:\n",
        "        logger (f\"remove_k {remove_k} is too large for {n_features} features, setting it to [n_features // 2] {n_features // 2}\")\n",
        "        remove_k = (ranking.shape[0]-1) // 2\n",
        "    if remove_k < 1:\n",
        "        logger (f\"remove_k {remove_k} is too small, setting it to 1\")\n",
        "        remove_k = 1\n",
        "\n",
        "    bests = ranking.index[:remove_k]\n",
        "    worsts = ranking.index[-remove_k:]\n",
        "\n",
        "    differences = []\n",
        "    for seed in tqdm(random_states, total=len(random_states)):\n",
        "        no_best = np.arange(n_features)\n",
        "        no_best = np.delete(no_best, bests)\n",
        "        no_worst = np.arange(n_features)\n",
        "        no_worst = np.delete(no_worst, worsts)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=int(n_samples * test_ratio),\n",
        "            stratify=y,\n",
        "            random_state=seed\n",
        "        )\n",
        "        \n",
        "        iforest = IsolationForest(n_estimators=100, random_state=seed)\n",
        "\n",
        "        iforest.fit(X_train[..., no_best])\n",
        "        no_best_decision = -iforest.decision_function(X_test[..., no_best])\n",
        "        no_best_ap = average_precision_score(y_test, no_best_decision)\n",
        "\n",
        "        iforest.fit(X_train[..., no_worst])\n",
        "        no_worst_decision = -iforest.decision_function(X_test[..., no_worst])\n",
        "        no_worst_ap = average_precision_score(y_test, no_worst_decision)\n",
        "\n",
        "        differences.append(no_worst_ap-no_best_ap)\n",
        "    return np.mean(differences), np.std(differences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log(message, file=None, verbose=False):\n",
        "    if verbose:\n",
        "        print(message)\n",
        "    if file is not None:\n",
        "        with open(file, 'a', encoding='utf-8')  as f:\n",
        "            f.write(message + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWxoRSmf788F"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_experiences = 3 \n",
        "n_seeds_exp = 50 \n",
        "n_blocks_per_experience = 13\n",
        "n_start_trees = 100\n",
        "n_trees_progression = [1,2,5,10,25,50,100]\n",
        "datasets_names = odds_datasets.datasets_names\n",
        "datasets = [odds_datasets.load(dataset_name=name, scale=False) for name in datasets_names]\n",
        "test_ratio = 0.2\n",
        "n_seeds_shap = 30\n",
        "n_seeds_worst_best = 30\n",
        "remove_k_worst_best = 2\n",
        "verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Samples</th>\n",
              "      <th>Features</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>wine</th>\n",
              "      <td>129</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vertebral</th>\n",
              "      <td>240</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ionosphere</th>\n",
              "      <td>351</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wbc</th>\n",
              "      <td>378</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breastw</th>\n",
              "      <td>683</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pima</th>\n",
              "      <td>768</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vowels</th>\n",
              "      <td>1456</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>letter</th>\n",
              "      <td>1600</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cardio</th>\n",
              "      <td>1831</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thyroid</th>\n",
              "      <td>3772</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optdigits</th>\n",
              "      <td>5216</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>satimage-2</th>\n",
              "      <td>5803</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>satellite</th>\n",
              "      <td>6435</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pendigits</th>\n",
              "      <td>6870</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annthyroid</th>\n",
              "      <td>7200</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mnist</th>\n",
              "      <td>7603</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mammography</th>\n",
              "      <td>11183</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Samples  Features\n",
              "Dataset                       \n",
              "wine             129        13\n",
              "vertebral        240         6\n",
              "ionosphere       351        33\n",
              "wbc              378        30\n",
              "breastw          683         9\n",
              "pima             768         8\n",
              "vowels          1456        12\n",
              "letter          1600        32\n",
              "cardio          1831        21\n",
              "thyroid         3772         6\n",
              "optdigits       5216        64\n",
              "satimage-2      5803        36\n",
              "satellite       6435        36\n",
              "pendigits       6870        16\n",
              "annthyroid      7200         6\n",
              "mnist           7603       100\n",
              "mammography    11183         6"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# plot of the number of samples and features in each dataset\n",
        "description_df = pd.DataFrame({\n",
        "    'Dataset': datasets_names,\n",
        "    'Samples': [X.shape[0] for X, _ in datasets],\n",
        "    'Features': [X.shape[1] for X, _ in datasets]\n",
        "})\n",
        "description_df.set_index('Dataset', inplace=True)\n",
        "description_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "dataset_index = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "current_dataset_key = datasets_names[dataset_index]\n",
        "\n",
        "results_dir = os.path.join(working_dir, f\"results_{current_dataset_key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make a folder for the results, don't overwrite otherwise it's easy to lose results\n",
        "if os.path.exists(results_dir):\n",
        "    raise ValueError(f\"Results directory {results_dir} already exists. Please choose a different directory.\")\n",
        "os.makedirs(results_dir, exist_ok=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWy4C_ZQo5lv",
        "outputId": "20e901f9-45b6-44a7-b511-d19fa03474af"
      },
      "outputs": [],
      "source": [
        "\n",
        "X, y = datasets[dataset_index]\n",
        "general_info_file = os.path.join(results_dir, f\"{current_dataset_key}_general_info.txt\")\n",
        "main_results_file = os.path.join(results_dir, f\"{current_dataset_key}_results.json\")\n",
        "secondary_results_file = os.path.join(results_dir, f\"{current_dataset_key}_secondary_results.json\")\n",
        "seeds_file = os.path.join(results_dir, f\"{current_dataset_key}_seeds.txt\")\n",
        "main_results = nested_defaultdict()\n",
        "secondary_results = nested_defaultdict()\n",
        "\n",
        "log(f\"Running experiments for dataset: {current_dataset_key}\", file=general_info_file, verbose=verbose)\n",
        "\"\"\"\n",
        "Log parameters\n",
        "\"\"\"\n",
        "log(\"Parameters:\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_experiences: {n_experiences}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_seeds_exp: {n_seeds_exp}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_blocks_per_experience: {n_blocks_per_experience}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_trees_progression: {n_trees_progression}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_start_trees: {n_start_trees}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# test_ratio: {test_ratio}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_seeds_shap: {n_seeds_shap}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# n_seeds_worst_best: {n_seeds_worst_best}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"# remove_k_worst_best: {remove_k_worst_best}\", file=general_info_file, verbose=verbose)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "GENERAL DESCRIPRION OF THE DATASET\n",
        "\"\"\"\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "log(f\"Number of samples: {n_samples}, Number of features: {n_features}\", file=general_info_file, verbose=verbose)\n",
        "log(f\"Original label distribution: {np.unique(y, return_counts=True)}\", file=general_info_file, verbose=verbose)\n",
        "\n",
        "# Try to infer which label is anomaly (usually the minority class)\n",
        "labels, counts = np.unique(y, return_counts=True)\n",
        "if len(labels) != 2:\n",
        "    raise ValueError(\"Expected a binary classification problem.\")\n",
        "# Assume smaller class is anomaly\n",
        "anomaly_label = labels[np.argmin(counts)]\n",
        "# just to avoid problems with \"same number of samples in both classes\"\n",
        "normal_label = np.setdiff1d(labels, anomaly_label)[0]  # The other label is normal\n",
        "# Map: 0 = normal, 1 = anomaly for model compatibility\n",
        "y_mapped = np.where(y == normal_label, 0, 1)\n",
        "log(f\"Mapped labels: normal={normal_label}→0, anomaly={anomaly_label}→1\", file=general_info_file, verbose=verbose)\n",
        "log(f\"Final label distribution: {np.unique(y_mapped, return_counts=True)}\", file=general_info_file, verbose=verbose)\n",
        "\n",
        "\"\"\"\n",
        "CHECK IF THE NUMBER OF FEATURES IS SUFFICIENT FOR THE EXPERIMENTS\n",
        "If the number of features is less than the number of experiences, reduce the number of experiences.\n",
        "\"\"\"\n",
        "\n",
        "actual_n_experiences = n_experiences\n",
        "if n_features < actual_n_experiences:\n",
        "    log(f\"Dataset {current_dataset_key} has too many experiences ({actual_n_experiences}) for {n_features} features.\", file=general_info_file, verbose=verbose)\n",
        "    actual_n_experiences = n_features - 1\n",
        "    log(f\"Reducing number of experiences to n_features - 1: {actual_n_experiences}\", file=general_info_file, verbose=verbose)\n",
        "\n",
        "\"\"\"\n",
        "SHAP TREE FEATURE RANKING AND GENERAL CHECKS\n",
        "\"\"\"\n",
        "\n",
        "oneshot_if_check_ap = isolation_forest_oneshot_AP_check(X, y_mapped, test_ratio)\n",
        "log(f\"One-shot Isolation Forest AP score: {oneshot_if_check_ap:.4f}\", file=general_info_file, verbose=verbose)\n",
        "\n",
        "ranking_seeds = np.random.randint(0, 10000, n_seeds_shap)\n",
        "log(f\"ranking_seeds: {ranking_seeds.tolist()}\", file=seeds_file, verbose=False)\n",
        "ranking = shaptree_feature_ranking(X, y_mapped, random_states=ranking_seeds, test_ratio=test_ratio)\n",
        "log(f\"Feature ranking based on SHAP values:\\n{ranking.to_string(index=True)}\", file=general_info_file, verbose=verbose)\n",
        "feature_order = np.flip(ranking.index)\n",
        "\n",
        "worst_best_seeds = np.random.randint(0, 10000, n_seeds_worst_best)\n",
        "log(f\"worst_best_seeds: {worst_best_seeds.tolist()}\", file=seeds_file, verbose=False)\n",
        "worst_best_ap_mean, worst_best_ap_std = test_remove_best_worst_ranking(\n",
        "    X, y_mapped, ranking, test_ratio,\n",
        "    random_states=worst_best_seeds,\n",
        "    remove_k=remove_k_worst_best,\n",
        "    logger=lambda msg: log(msg, file=general_info_file, verbose=verbose)\n",
        ")\n",
        "log(\n",
        "    f\"AP difference after removing {remove_k_worst_best} best and worst features: mean={worst_best_ap_mean:.4f}, std={worst_best_ap_std:.4f}\",\n",
        "    file=general_info_file, verbose=verbose\n",
        ")\n",
        "\n",
        "experiment_seeds = np.random.randint(0, 10000, n_seeds_exp)\n",
        "log(f\"experiment_seeds: {experiment_seeds.tolist()}\", file=seeds_file, verbose=False)\n",
        "\n",
        "for n_new_trees in n_trees_progression:\n",
        "    for seed in experiment_seeds:\n",
        "        log(f\"Running experiment with n_new_trees={n_new_trees} and seed={seed}...\", file=general_info_file, verbose=verbose)\n",
        "        main_out, sec_out = run_isolation_forest_experiment(\n",
        "            X=X,\n",
        "            y=y_mapped,\n",
        "            num_experiences=n_experiences,\n",
        "            num_blocks_per_experience=n_blocks_per_experience,\n",
        "            n_new_trees=n_new_trees,\n",
        "            contamination=0.1, # this shouldn't matter as we are using average precision\n",
        "            feature_permutation=feature_order,\n",
        "            num_test_points=int(n_samples * test_ratio),\n",
        "            model_class=RootForcedIForest,\n",
        "            random_state=seed,\n",
        "            n_start_trees=n_start_trees\n",
        "        )\n",
        "        main_results[f\"n_new_trees:{n_new_trees}\"][f\"seed:{seed}\"] = main_out\n",
        "        secondary_results[f\"n_new_trees:{n_new_trees}\"][f\"seed:{seed}\"] = sec_out\n",
        "\n",
        "with open(main_results_file, 'w') as f:\n",
        "    json.dump(main_results, f, indent=4)\n",
        "with open(secondary_results_file, 'w') as f:\n",
        "    json.dump(secondary_results, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_dict = {\n",
        "    \"fixed\": \"#003a7d\",\n",
        "    \"from_scratch\": \"#d83034\",\n",
        "    \"from_scratch_2\": \"#ff73b6\",\n",
        "    \"from_scratch_iso\": \"#5ecaba\",\n",
        "    \"ours_none\": \"#008dff\",\n",
        "    \"ours_force\": \"#f67f00\",\n",
        "    \"ours_diffi\": \"#cf23ff\",\n",
        "    \"ours_keepsize\": \"#935610\",\n",
        "    \"ours_force_diffi\": \"#3fa271\",\n",
        "    \"ours_keepsize_diffi\": \"#c6cd00\",\n",
        "    \"ours_force_keepsize\": \"#5F6D7B\",\n",
        "    \"ours_all\": \"#611EA4\"\n",
        "    }\n",
        "lines_dict = {\n",
        "    \"fixed\": \"-\",\n",
        "    \"from_scratch\": \"-\",\n",
        "    \"from_scratch_2\": \"--\",\n",
        "    \"from_scratch_iso\": \"--\",\n",
        "    \"ours_none\": \"--\",\n",
        "    \"ours_force\": \"--\",\n",
        "    \"ours_diffi\": \"--\",\n",
        "    \"ours_keepsize\": \"--\",\n",
        "    \"ours_force_diffi\": \"--\",\n",
        "    \"ours_keepsize_diffi\": \"--\",\n",
        "    \"ours_force_keepsize\": \"--\",\n",
        "    \"ours_all\": \"--\"\n",
        "}\n",
        "\n",
        "plot_folder = os.path.join(results_dir, f\"{current_dataset_key}_plots\")\n",
        "secondary_plot_folder = os.path.join(results_dir, f\"{current_dataset_key}_secondary_plots\")\n",
        "time_plot_folder = os.path.join(results_dir, f\"{current_dataset_key}_time_plots\")\n",
        "\n",
        "\n",
        "main_data = json.load(open(os.path.join(results_dir, f\"{current_dataset_key}_results.json\"), 'r'))\n",
        "secondary_data = json.load(open(os.path.join(results_dir, f\"{current_dataset_key}_secondary_results.json\"), 'r'))\n",
        "    \n",
        "plot_experiment_results(\n",
        "    main_data, \n",
        "    save_dir=plot_folder,\n",
        "    dataset_name=current_dataset_key,\n",
        "    model_colors=color_dict,\n",
        "    model_lines=lines_dict\n",
        ")\n",
        "plot_experiment_results(\n",
        "    secondary_data, \n",
        "    save_dir=secondary_plot_folder,\n",
        "    dataset_name=current_dataset_key,\n",
        "    model_colors=color_dict,\n",
        "    model_lines=lines_dict\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
